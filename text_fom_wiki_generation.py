''' IMPORTS '''
import os
import re
import sys
import json
import time
import random
import numpy as np
from urllib import request
from keras import optimizers
from keras.models import Sequential
from bs4 import BeautifulSoup as bs
from keras.layers import Dense, Activation, LSTM, Dropout


# BASE : https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py
# SOURCE : https://github.com/vlraik/word-level-rnn-keras/blob/master/wordlevelrnn/__init__.py
# DOCU : http://karpathy.github.io/2015/05/21/rnn-effectiveness/

''' PARAMETERS '''
param = {
	'language': 'fr',				# Language of generated text
	'wiki_article_number': 10,		# Number of articles to create dataset
	'window_size' : 3,				# Split text into bag of x words
	'sliding' : 1,					# Slide every x words
	'iteration' : 2,				# Number of iterations
	'text_size' : 300,				# Size of the generated text
	'batch_size' : 128,				# Number of BOW parsed at a time
	'epochs' : 100,					# How many epochs
	'learning_rate' : 0.01			# Learning rate for optimization
}


''' GENERAL NEEDS '''
timeStart = time.time()
print('- ' * 50)

''' REMOVE HTML TAG AROUND INPUT TEXT '''
def cleanhtml(raw_html):
  cleantext = re.sub('<.*?>', '', str(raw_html))
  return cleantext

''' EXTRACT A RANDOM ARTICLE FROM WIKIPEDIA '''
def getWiki():
	if param['language'] == 'fr':
		randomUrl = 'https://fr.wikipedia.org/wiki/Sp%C3%A9cial:Page_au_hasard'
	else:
		randomUrl = 'https://en.wikipedia.org/wiki/Special:Random'
	# Get random page and extract URL
	randomPageData = request.urlopen(randomUrl).read().decode('utf-8')
	randomPageSoup = bs(randomPageData, 'html.parser')
	url = re.findall('<link href="(.*)" rel="canonical"/>', str(randomPageSoup))[0]
	# Scrap this URL
	try:
		pageData = request.urlopen(url).read().decode('utf-8')
	except:
		print('Error while downloading data.')
	totArticles.append(url)
	pageSoup = bs(pageData, 'html.parser')
	rText = cleanhtml(pageSoup.find_all('p')).lower()
	return rText, totArticles

''' CREATE A DATASET WITH param['wikiArticleNumber'] ARTICLES '''
totText = []
totArticles = []
while len(totText) < param['wiki_article_number']:
	textToAdd = getWiki()[0]
	totText.append(textToAdd)
print('Number of articles extracted from Wikipedia : {}\n{}'.format(len(totText), totArticles))
# Join every list items in a big text
text = ' '.join(totText)
print('Total number of characters : {}'.format(len(text)))

''' TEXT PROCESSING '''
chars = set(text)
# Split into a BOW
words = set(text.lower().split())
print('Number of unique terms : {}'.format(len(words)))
print('Number of alphanumeric characters : {}'.format(len(chars)))
# Create two dictionnaries for words indices
word_indices = dict((c, i) for i, c in enumerate(words))
indices_word = dict((i, c) for i, c in enumerate(words))
# Sliding windows parameters
maxlen = param['window_size']
step = param['sliding']
print('Widonws of {} word, sliding every {} terms'.format(maxlen, step))
# Empty list to keep BOW
sentences = []
next_words = []
next_words= []
sentences1 = []
list_words = []
sentences2=[]
list_words=text.lower().split()
# Process BOW into corpus generated by sliding window
for i in range(0,len(list_words)-maxlen, step):
	sentences2 = ' '.join(list_words[i: i + maxlen])
	sentences.append(sentences2)
	next_words.append((list_words[i + maxlen]))  
print('Number of sequences generaed by the sliding window : {}'.format(len(sentences)))
# Vectorisation into numpy array of [0,1]. Boolean fasters ?
X = np.zeros((len(sentences), maxlen, len(words)), dtype=np.int)
y = np.zeros((len(sentences), len(words)), dtype=np.int)
for i, sentence in enumerate(sentences):
	for t, word in enumerate(sentence.split()):
		X[i, t, word_indices[word]] = 1
	y[i, word_indices[next_words[i]]] = 1
# Build our two hidden layers model 
model = Sequential()
model.add(LSTM(256, return_sequences=True, input_shape=(maxlen, len(words))))
model.add(Dropout(0.2))
model.add(LSTM(256, return_sequences=False))
model.add(Dropout(0.5))
model.add(Dense(len(words)))
#model.add(Dense(1000))
model.add(Activation('softmax'))
# Optimizers. Doc : https://keras.io/optimizers/
'''
RMSProp = optimizers.RMSprop(lr=param['learning_rate'])
adam = optimizers.Adam(lr=param['learning_rate'], beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)
sgd = optimizers.SGD(lr=param['learning_rate'], momentum=0.0, decay=0.0, nesterov=False)
'''
adam = optimizers.Adam(lr=param['learning_rate'], beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)
model.compile(loss='categorical_crossentropy', optimizer=adam)
# Timestamp
print('Dataset extracted and model compiled in {} sec.'.format(round(time.time()-timeStart, 2)))

# Checkpointing
#if os.path.isfile('GoTweights'):
#    model.load_weights('GoTweights')

# Temperature. If < 1, model get stuck in a loop of vocabulary
def sample(preds, temperature=1.0):
	# helper function to sample an index from a probability array
	preds = np.asarray(preds).astype('float64')
	preds = np.log(preds) / temperature
	exp_preds = np.exp(preds)
	preds = exp_preds / np.sum(exp_preds)
	probas = np.random.multinomial(1, preds, 1)
	return np.argmax(probas)

''' TRAINING AND OUTPUT '''
for iteration in range(1, param['iteration']):
	print('- ' * 50)
	print('Iteration', iteration)
	# Fit the model
	model.fit(X, y, 
		batch_size = param['batch_size'], 
		epochs = param['epochs'],
		verbose = 1)   
	# Checkpointing
	#model.save_weights('GoTweights',overwrite=True)
	start_index = random.randint(0, len(list_words) - maxlen - 1)
	# Diversity too loow, model get stuck. Too high, get crazy.
	for diversity in [1.0]:
		print()
		print('Diversity:', diversity)
		generated = ''
		sentence = list_words[start_index: start_index + maxlen]
		generated += ' '.join(sentence)
		print('Generating with seed: "' , generated , '"')
		print('- ' * 50)
		# Generate output text
		for i in range(param['text_size']):
			x = np.zeros((1, maxlen, len(words)))
			for t, word in enumerate(sentence):
				x[0, t, word_indices[word]] = 1.
			preds = model.predict(x, verbose=0)[0]
			next_index = sample(preds, diversity)
			next_word = indices_word[next_index]
			generated += next_word
			del sentence[0]
			
			sentence.append(next_word)
			sys.stdout.write(' ')
			sys.stdout.write(next_word)
			sys.stdout.flush()
		print()
		print('- ' * 50)
# Checkpointing
#model.save_weights('weights') 
print('Text generated in {} sec.'.format(round(time.time()-timeStart, 2)))

