#!/usr/bin/python3


import os
import re
import sys
import json
import time
import string
import random
import numpy as np
import configparser
from urllib import request
from keras import optimizers
from keras.models import Sequential
from bs4 import BeautifulSoup as bs
from keras.layers import Dense, Activation, LSTM, Dropout


# BASE : https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py
# SOURCE : https://github.com/vlraik/word-level-rnn-keras/blob/master/wordlevelrnn/__init__.py
# DOCU : http://karpathy.github.io/2015/05/21/rnn-effectiveness/


class Timer(object):
	def __init__(self):
		"""Return actual timestamp"""
		self.t = time.time()


class Helpers(object):
	def __init__(self, config):
		"""Data helpers"""
		self.config = config

	def clean_html(self, raw_html):
		"""Remove XML"""
		cleantext = re.sub('<.*?>', '', str(raw_html))
		return cleantext

	def clean_punctuation(self, raw_string):
		""""""
		cleaned_string = str(raw_string.translate(str.maketrans('', '', string.punctuation)))
		return cleaned_string


class WikiData(object):
	def __init__(self, config):
		""""""
		self.config = config

	def get_wiki(self, totText, totArticles):
		"""Extract a random article from Wiki"""
		if self.config['WIKIPEDIA']['language'] == 'fr':
			randomUrl = 'https://fr.wikipedia.org/wiki/Sp%C3%A9cial:Page_au_hasard'
		else:
			randomUrl = 'https://en.wikipedia.org/wiki/Special:Random'
		randomPageData = request.urlopen(randomUrl).read().decode('utf-8')  # Get random page and extract URL
		randomPageSoup = bs(randomPageData, 'html.parser')
		url = re.findall('<link href="(.*)" rel="canonical"/>', str(randomPageSoup))[0]
		try:
			pageData = request.urlopen(url).read().decode('utf-8')  # Scrap this URL
		except Exception as E:
			print('Error while downloading data ({}).'.format(E))
		totArticles.append(url)
		pageSoup = bs(pageData, 'html.parser')
		data_helper = Helpers(config=self.config)
		rText = data_helper.clean_html(pageSoup.find_all('p')).lower()
		totText.append(rText)
		return totText, totArticles

	def preprocess_text(self, text, words):
		"""Handle X and y creation for training"""
		word_indices = dict((c, i) for i, c in enumerate(words))  # Create two dictionnaries for words indices
		indices_word = dict((i, c) for i, c in enumerate(words))
		maxlen = self.config.getint('WIKIPEDIA', 'window_size')  # Sliding windows parameters
		step = self.config.getint('WIKIPEDIA', 'sliding')
		print('Widonws of {} word, sliding every {} terms'.format(maxlen, step))
		sentences = []  # Empty list to keep BOW created via sliding method
		next_words = []
		list_words = []
		list_words=text.lower().split()
		for i in range(0,len(list_words)-maxlen, step):  # Process BOW into corpus generated by sliding window
			sentences_window = ' '.join(list_words[i: i + maxlen])
			sentences.append(sentences_window)
			next_words.append((list_words[i + maxlen]))  
		print('Number of sequences generaed by the sliding window : {}'.format(len(sentences)))
		X = np.zeros((len(sentences), maxlen, len(words)), dtype=np.int)  # Vectorisation into numpy array of [0,1]. Boolean fasters ?
		y = np.zeros((len(sentences), len(words)), dtype=np.int)
		for i, sentence in enumerate(sentences):
			for t, word in enumerate(sentence.split()):
				X[i, t, word_indices[word]] = 1
			y[i, word_indices[next_words[i]]] = 1
		return X, y


class LSTM(object):
	def __init__(self, config):
		""""""
		self.config = config

	def initialize(self, dense_shape):
		"""Return a sweeet and nice Keras LSTM"""
		lstmmodel = Sequential()  # Build our two hidden layers model

		lstmmodel.add(LSTM(256, return_sequences=True, input_shape=(self.config.getint('WIKIPEDIA', 'window_size'), dense_shape)))
		lstmmodel.add(Dropout(0.2))
		lstmmodel.add(LSTM(256, return_sequences=False))
		lstmmodel.add(Dropout(0.5))
		lstmmodel.add(Dense(dense_shape))
		lstmmodel.add(Activation('softmax'))

		# Optimizers. Doc : https://keras.io/optimizers/
		# RMSProp = optimizers.RMSprop(lr=param['learning_rate'])
		# adam = optimizers.Adam(lr=param['learning_rate'], beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)
		# sgd = optimizers.SGD(lr=param['learning_rate'], momentum=0.0, decay=0.0, nesterov=False)

		adam = optimizers.Adam(lr=param['learning_rate'], beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)  # Parameters to loop over for hyperparameter optimisation (gridSearch)
		lstmmodel.compile(loss='categorical_crossentropy', optimizer=adam)
		return lstmmodel

	def train(self, model):
		"""Train that shit with fit()"""
		for iteration in range(1, config['LSTM']['iteration']):
			print('Iteration {}/{}'.format(iteration, config['LSTM']['iteration'] - 1))
			model.fit(X, y,  # Fit the model
				batch_size = param['batch_size'], 
				epochs = param['epochs'],
				verbose = 1)
		return model

	def sample_temp(preds, temperature=1.0):
		"""If < 1, model get stuck in a loop of vocabulary"""
		preds = np.asarray(preds).astype('float64')
		preds = np.log(preds) / temperature
		exp_preds = np.exp(preds)
		preds = exp_preds / np.sum(exp_preds)
		probas = np.random.multinomial(1, preds, 1)
		return np.argmax(probas)

	def test_temperature(self, lstm_model, temp_list, text):
		""""""
		
		maxlen = self.config.getint('WIKIPEDIA', 'window_size')  # Sliding windows parameters
		list_words=text.lower().split()
		start_index = random.randint(0, len(list_words) - maxlen - 1)

		for diversity in temp_list:  # Diversity too loow, model get stuck. Too high, get crazy.
			print()
			print('Diversity:', diversity)
			generated = ''
			sentence = list_words[start_index: start_index + maxlen]
			generated += ' '.join(sentence)
			print('Generating with seed: "' , generated , '"')
			print('- ' * 50)
			# Generate output text
			for i in range(param['text_size']):
				x = np.zeros((1, maxlen, len(words)))
				for t, word in enumerate(sentence):
					x[0, t, word_indices[word]] = 1.
				preds = lstm_model.predict(x, verbose=0)[0]
				next_index = lstm_model.sample_temp(preds, diversity)
				next_word = indices_word[next_index]
				generated += next_word
				del sentence[0]
				sentence.append(next_word)
				sys.stdout.write(' ')
				sys.stdout.write(next_word)
				sys.stdout.flush()
			print()
			print('- ' * 50)



if __name__ == '__main__':

	timer = Timer()

	config = configparser.ConfigParser()
	config.read('./configuration.cfg')

	data_helper = Helpers(config=config)
	wiki_handler = WikiData(config=config)
	lstm_model = LSTM(config=config)

	totText = []; totArticles = []
	while len(totText) < config.getint('WIKIPEDIA', 'wiki_article_number'):  # Loop to find X articles
		totText, totArticles = wiki_handler.get_wiki(totText, totArticles)
	print('Number of articles extracted from Wikipedia : {}\n\t- {}\n'.format(len(totText), ('\n\t- '.join(totArticles))))
	text = ' '.join(totText)  # Join every list items in a big text
	text_cleaned = data_helper.clean_punctuation(raw_string=text)
	print('Total number of characters : {} ({} before punctuation clean)'.format(len(text_cleaned), len(text)))

	chars = set(text)
	words = set(text.lower().split())  # Better than re.findall('\w+) here because of ' kept while splitting on the space
	print('Number of unique terms : {}\nNumber of alphanumeric characters : {}'.format(len(words), len(chars)))

	X, y = wiki_handler.preprocess_text(text=text, words=words)
	print('\n\tX:\t{}\n\ty:\t{}'.format(np.shape(X), np.shape(y)))

	model = lstm_model.initialize(dense_shape=len(words))  # Init
	model = lstm_model.train(model=model)  # Train
	model = lstm.test_temperature(lstm_model=model, temp_list=[0.5, 0.8, 1.0, 1.5, 1.8], text=text)

